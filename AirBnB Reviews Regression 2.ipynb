{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import altair as alt\n",
    "import gzip\n",
    "import shutil\n",
    "import pathlib\n",
    "import os\n",
    "import sqlalchemy\n",
    "import sqlite3\n",
    "import spacy\n",
    "import re\n",
    "import spacy_cleaner\n",
    "from spacy_cleaner import processing, Cleaner\n",
    "import re\n",
    "import tqdm\n",
    "from tqdm.notebook import tqdm, trange\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import IntProgress, HTML, VBox\n",
    "from IPython.display import display\n",
    "import time\n",
    "import timeit\n",
    "import sklearn\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import cupy as cp\n",
    "import xgboost\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "import torch\n",
    "from torch.nn.functional import softmax\n",
    "\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "sid_obj = SentimentIntensityAnalyzer()\n",
    "\n",
    "%matplotlib inline\n",
    "alt.data_transformers.disable_max_rows()\n",
    "\n",
    "\n",
    "#spacy.prefer_gpu()\n",
    "spacy.require_gpu(gpu_id=0)\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# This loads a larger and more robust model. Use with caution though because it takes considerably longer to run\n",
    "#nlp = spacy.load(\"en_core_web_trf\")\n",
    "\n",
    "#nlp = spacy.load('/path/to/en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "def print_files_in_directory(directory_path):\n",
    "    with os.scandir(directory_path) as entries:\n",
    "        for entry in entries:\n",
    "            if entry.is_file():\n",
    "                print(entry.name)\n",
    "\n",
    "def get_sentiment_score(text):\n",
    "    sentiment_scores = sid_obj.polarity_scores(text)\n",
    "    return sentiment_scores\n",
    "\n",
    "def clean_text(text):\n",
    "    doc = nlp(text)\n",
    "\n",
    "    cleaned_tokens = [token.lemma_.lower().strip() for token in doc if not token.is_punct and not token.is_space]\n",
    "    cleaned_tokens = [token for token in cleaned_tokens if not nlp.vocab[token].is_stop]\n",
    "    cleaned_text = ' '.join(cleaned_tokens)\n",
    "\n",
    "    return cleaned_text\n",
    "\n",
    "def remove_numbers(text):\n",
    "    cleaner = Cleaner(\n",
    "        nlp,\n",
    "        processing.remove_number_token\n",
    "    )\n",
    "    return cleaner.clean(text)\n",
    "\n",
    "def remove_numbers_regex(text):\n",
    "    # Pattern to remove numbers from text data\n",
    "    pattern = r\"\\d+\"\n",
    "\n",
    "    return re.sub(pattern,\"\",text)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "conn = sqlite3.connect(r\"C:/Users/asl4a/AirBnB_Data.db\")\n",
    "cursor = conn.cursor()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "   Unnamed: 0  temp_index  listing_id     id        date  reviewer_id  \\\n0           0           0        6422   1927  2009-04-30        14100   \n1           1           1        6422   3867  2009-06-11        17413   \n2           2           2        6422   4159  2009-06-17        20253   \n3           3           3        6422   5724  2009-07-18        22544   \n4           4           4        6422  11891  2009-09-29        33409   \n\n  reviewer_name                                           comments  \\\n0       Melissa  I can't say enough about how wonderful it was ...   \n1        Raquel  Michelle and Collier's home is wonderful! They...   \n2        Ulrike  I spent one night at Michele's home and felt j...   \n3          Phil  Michele and Collier are two of the loveliest p...   \n4        Claire  We had the most lovely time staying with Miche...   \n\n                                        cleaned_text  \\\n0  i cant say enough about how wonderful it was t...   \n1  michelle and colliers home is wonderful they a...   \n2  i spent one night at micheles home and felt ju...   \n3  michele and collier are two of the loveliest p...   \n4  we had the most lovely time staying with miche...   \n\n                                              tokens  \n0  ['not', 'wonderful', 'stay', 'highlight', 'sta...  \n1  ['michelle', 'collier', 'home', 'wonderful', '...  \n2  ['spend', 'night', 'micheles', 'home', 'feel',...  \n3  ['michele', 'collier', 'lovely', 'people', 'pl...  \n4  ['lovely', 'time', 'stay', 'michele', 'colly',...  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Unnamed: 0</th>\n      <th>temp_index</th>\n      <th>listing_id</th>\n      <th>id</th>\n      <th>date</th>\n      <th>reviewer_id</th>\n      <th>reviewer_name</th>\n      <th>comments</th>\n      <th>cleaned_text</th>\n      <th>tokens</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>0</td>\n      <td>6422</td>\n      <td>1927</td>\n      <td>2009-04-30</td>\n      <td>14100</td>\n      <td>Melissa</td>\n      <td>I can't say enough about how wonderful it was ...</td>\n      <td>i cant say enough about how wonderful it was t...</td>\n      <td>['not', 'wonderful', 'stay', 'highlight', 'sta...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>1</td>\n      <td>6422</td>\n      <td>3867</td>\n      <td>2009-06-11</td>\n      <td>17413</td>\n      <td>Raquel</td>\n      <td>Michelle and Collier's home is wonderful! They...</td>\n      <td>michelle and colliers home is wonderful they a...</td>\n      <td>['michelle', 'collier', 'home', 'wonderful', '...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>2</td>\n      <td>6422</td>\n      <td>4159</td>\n      <td>2009-06-17</td>\n      <td>20253</td>\n      <td>Ulrike</td>\n      <td>I spent one night at Michele's home and felt j...</td>\n      <td>i spent one night at micheles home and felt ju...</td>\n      <td>['spend', 'night', 'micheles', 'home', 'feel',...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>3</td>\n      <td>6422</td>\n      <td>5724</td>\n      <td>2009-07-18</td>\n      <td>22544</td>\n      <td>Phil</td>\n      <td>Michele and Collier are two of the loveliest p...</td>\n      <td>michele and collier are two of the loveliest p...</td>\n      <td>['michele', 'collier', 'lovely', 'people', 'pl...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>4</td>\n      <td>6422</td>\n      <td>11891</td>\n      <td>2009-09-29</td>\n      <td>33409</td>\n      <td>Claire</td>\n      <td>We had the most lovely time staying with Miche...</td>\n      <td>we had the most lovely time staying with miche...</td>\n      <td>['lovely', 'time', 'stay', 'michele', 'colly',...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_data_dir_path = r\"F:\\Data Science\\Datasets\\Cleaned Reviews\"\n",
    "\n",
    "cleaned_reviews_list = []\n",
    "\n",
    "with os.scandir(cleaned_data_dir_path) as entries:\n",
    "    for entry in entries:\n",
    "        if entry.is_file():\n",
    "            cleaned_reviews_list.append(pd.read_csv(entry.path))\n",
    "\n",
    "cleaned_revs_df = pd.concat(cleaned_reviews_list)\n",
    "cleaned_revs_df.head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 10668047 entries, 0 to 1668044\n",
      "Data columns (total 10 columns):\n",
      " #   Column         Dtype \n",
      "---  ------         ----- \n",
      " 0   Unnamed: 0     int64 \n",
      " 1   temp_index     int64 \n",
      " 2   listing_id     int64 \n",
      " 3   id             int64 \n",
      " 4   date           object\n",
      " 5   reviewer_id    int64 \n",
      " 6   reviewer_name  object\n",
      " 7   comments       object\n",
      " 8   cleaned_text   object\n",
      " 9   tokens         object\n",
      "dtypes: int64(5), object(5)\n",
      "memory usage: 895.3+ MB\n"
     ]
    }
   ],
   "source": [
    "cleaned_revs_df.info()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9668066\n"
     ]
    }
   ],
   "source": [
    "print(len(cleaned_revs_df.drop_duplicates()))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "sql = \"\"\"\n",
    "SELECT\n",
    "    l.host_id,\n",
    "    l.id,\n",
    "    l.review_scores_rating,\n",
    "    l.review_scores_accuracy,\n",
    "    l.review_scores_cleanliness,\n",
    "    l.review_scores_checkin,\n",
    "    l.review_scores_communication,\n",
    "    l.review_scores_location,\n",
    "    l.review_scores_value\n",
    "FROM\n",
    "    listings l;\n",
    "    \"\"\"\n",
    "\n",
    "listings_df = pd.read_sql(sql=sql,con=conn)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 228701 entries, 0 to 228700\n",
      "Data columns (total 9 columns):\n",
      " #   Column                       Non-Null Count   Dtype  \n",
      "---  ------                       --------------   -----  \n",
      " 0   host_id                      228701 non-null  int64  \n",
      " 1   id                           228701 non-null  int64  \n",
      " 2   review_scores_rating         179233 non-null  float64\n",
      " 3   review_scores_accuracy       178325 non-null  float64\n",
      " 4   review_scores_cleanliness    178339 non-null  float64\n",
      " 5   review_scores_checkin        178313 non-null  float64\n",
      " 6   review_scores_communication  178333 non-null  float64\n",
      " 7   review_scores_location       178306 non-null  float64\n",
      " 8   review_scores_value          178305 non-null  float64\n",
      "dtypes: float64(7), int64(2)\n",
      "memory usage: 15.7 MB\n"
     ]
    }
   ],
   "source": [
    "listings_df.info()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "df_revs = pd.concat([listings_df,cleaned_revs_df],keys=['listing_id','id'])\n",
    "#df_revs = pd.merge(listings_df,cleaned_revs_df,right_on='listing_id',left_on='id)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "MultiIndex: 10896748 entries, ('listing_id', 0) to ('id', 1668044)\n",
      "Data columns (total 18 columns):\n",
      " #   Column                       Dtype  \n",
      "---  ------                       -----  \n",
      " 0   host_id                      float64\n",
      " 1   id                           int64  \n",
      " 2   review_scores_rating         float64\n",
      " 3   review_scores_accuracy       float64\n",
      " 4   review_scores_cleanliness    float64\n",
      " 5   review_scores_checkin        float64\n",
      " 6   review_scores_communication  float64\n",
      " 7   review_scores_location       float64\n",
      " 8   review_scores_value          float64\n",
      " 9   Unnamed: 0                   float64\n",
      " 10  temp_index                   float64\n",
      " 11  listing_id                   float64\n",
      " 12  date                         object \n",
      " 13  reviewer_id                  float64\n",
      " 14  reviewer_name                object \n",
      " 15  comments                     object \n",
      " 16  cleaned_text                 object \n",
      " 17  tokens                       object \n",
      "dtypes: float64(12), int64(1), object(5)\n",
      "memory usage: 1.6+ GB\n"
     ]
    }
   ],
   "source": [
    "df_revs.info()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "df_revs['cleaned_text'] = df_revs['cleaned_text'].astype(str)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [],
   "source": [
    "#df_revs = df_revs.dropna(how='any',axis=0)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "# We have to rename the key in the listings dataset because when pandas merge function looks to see if there are keys with matching names in each dataset and was throwing errors when I tried to match on two different named keys even though they were the same values\n",
    "# Obviously this isn't the same as with raw SQL joins\n",
    "listings_df = listings_df.rename(columns={'id': 'listing_id'})"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "df_revs = pd.merge(listings_df,cleaned_revs_df,how='left',on=['listing_id','listing_id'])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "data": {
      "text/plain": "host_id                        0.000000\nlisting_id                     0.000000\nreview_scores_rating           0.461560\nreview_scores_accuracy         0.472841\nreview_scores_cleanliness      0.472673\nreview_scores_checkin          0.472990\nreview_scores_communication    0.472747\nreview_scores_location         0.473102\nreview_scores_value            0.473111\nUnnamed: 0                     0.461560\ntemp_index                     0.461560\nid                             0.461560\ndate                           0.461560\nreviewer_id                    0.461560\nreviewer_name                  0.461709\ncomments                       0.487153\ncleaned_text                   0.909656\ntokens                         0.461560\ndtype: float64"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_revs.isnull().sum() / len(df_revs) * 100\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "df_revs = df_revs.dropna(how='any',axis=0)\n",
    "#df_revs = df_revs.replace(np.nan, '')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "data": {
      "text/plain": "            host_id          listing_id  review_scores_rating  \\\n0             12172                6422                  4.95   \n1             12172                6422                  4.95   \n2             12172                6422                  4.95   \n3             12172                6422                  4.95   \n4             12172                6422                  4.95   \n...             ...                 ...                   ...   \n10717282   31058129  903053207440059523                  5.00   \n10717283   31058129  903053207440059523                  5.00   \n10717321  134981485  904459169206385872                  5.00   \n10717359  364080225  903836186965131941                  5.00   \n10717538   24594764  905798817363097610                  5.00   \n\n          review_scores_accuracy  review_scores_cleanliness  \\\n0                           4.94                       4.96   \n1                           4.94                       4.96   \n2                           4.94                       4.96   \n3                           4.94                       4.96   \n4                           4.94                       4.96   \n...                          ...                        ...   \n10717282                    5.00                       5.00   \n10717283                    5.00                       5.00   \n10717321                    5.00                       5.00   \n10717359                    5.00                       5.00   \n10717538                    5.00                       5.00   \n\n          review_scores_checkin  review_scores_communication  \\\n0                          4.97                         4.96   \n1                          4.97                         4.96   \n2                          4.97                         4.96   \n3                          4.97                         4.96   \n4                          4.97                         4.96   \n...                         ...                          ...   \n10717282                   5.00                         5.00   \n10717283                   5.00                         5.00   \n10717321                   5.00                         5.00   \n10717359                   5.00                         5.00   \n10717538                   5.00                         5.00   \n\n          review_scores_location  review_scores_value  Unnamed: 0  temp_index  \\\n0                           4.92                 4.98         0.0         0.0   \n1                           4.92                 4.98         1.0         1.0   \n2                           4.92                 4.98         2.0         2.0   \n3                           4.92                 4.98         3.0         3.0   \n4                           4.92                 4.98         4.0         4.0   \n...                          ...                  ...         ...         ...   \n10717282                    5.00                 5.00   9668038.0   1459398.0   \n10717283                    5.00                 5.00   9668039.0   1459399.0   \n10717321                    5.00                 5.00   9668040.0   1459400.0   \n10717359                    5.00                 5.00   9668036.0   1459396.0   \n10717538                    5.00                 5.00   9668044.0   1459404.0   \n\n                    id        date  reviewer_id reviewer_name  \\\n0         1.927000e+03  2009-04-30      14100.0       Melissa   \n1         3.867000e+03  2009-06-11      17413.0        Raquel   \n2         4.159000e+03  2009-06-17      20253.0        Ulrike   \n3         5.724000e+03  2009-07-18      22544.0          Phil   \n4         1.189100e+04  2009-09-29      33409.0        Claire   \n...                ...         ...          ...           ...   \n10717282  9.059651e+17  2023-06-03  266498612.0          Paul   \n10717283  9.073184e+17  2023-06-05     754306.0      Katerine   \n10717321  9.066321e+17  2023-06-04  233078624.0     Chien-Hao   \n10717359  9.066552e+17  2023-06-04   73264277.0     Catherine   \n10717538  9.073806e+17  2023-06-05  277166716.0        Lionel   \n\n                                                   comments  \\\n0         I can't say enough about how wonderful it was ...   \n1         Michelle and Collier's home is wonderful! They...   \n2         I spent one night at Michele's home and felt j...   \n3         Michele and Collier are two of the loveliest p...   \n4         We had the most lovely time staying with Miche...   \n...                                                     ...   \n10717282  Jordan is a great host, is super communicative...   \n10717283  I had an absolutely wonderful stay at Jordan’s...   \n10717321  A very smooth stay in this place. The host is ...   \n10717359  Nice private room with private bathroom. Easy ...   \n10717538  Omg. This place was absolutely amazing. Very s...   \n\n                                               cleaned_text  \\\n0         i cant say enough about how wonderful it was t...   \n1         michelle and colliers home is wonderful they a...   \n2         i spent one night at micheles home and felt ju...   \n3         michele and collier are two of the loveliest p...   \n4         we had the most lovely time staying with miche...   \n...                                                     ...   \n10717282  jordan is a great host is super communicative ...   \n10717283  i had an absolutely wonderful stay at jordans ...   \n10717321  a very smooth stay in this place the host is v...   \n10717359  nice private room with private bathroom easy t...   \n10717538  omg this place was absolutely amazing very spa...   \n\n                                                     tokens  \n0         ['not', 'wonderful', 'stay', 'highlight', 'sta...  \n1         ['michelle', 'collier', 'home', 'wonderful', '...  \n2         ['spend', 'night', 'micheles', 'home', 'feel',...  \n3         ['michele', 'collier', 'lovely', 'people', 'pl...  \n4         ['lovely', 'time', 'stay', 'michele', 'colly',...  \n...                                                     ...  \n10717282  ['jordan', 'great', 'host', 'super', 'communic...  \n10717283  ['absolutely', 'wonderful', 'stay', 'jordans',...  \n10717321  ['smooth', 'stay', 'place', 'host', 'responsiv...  \n10717359  ['nice', 'private', 'room', 'private', 'bathro...  \n10717538  ['omg', 'place', 'absolutely', 'amazing', 'spa...  \n\n[10618706 rows x 18 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>host_id</th>\n      <th>listing_id</th>\n      <th>review_scores_rating</th>\n      <th>review_scores_accuracy</th>\n      <th>review_scores_cleanliness</th>\n      <th>review_scores_checkin</th>\n      <th>review_scores_communication</th>\n      <th>review_scores_location</th>\n      <th>review_scores_value</th>\n      <th>Unnamed: 0</th>\n      <th>temp_index</th>\n      <th>id</th>\n      <th>date</th>\n      <th>reviewer_id</th>\n      <th>reviewer_name</th>\n      <th>comments</th>\n      <th>cleaned_text</th>\n      <th>tokens</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>12172</td>\n      <td>6422</td>\n      <td>4.95</td>\n      <td>4.94</td>\n      <td>4.96</td>\n      <td>4.97</td>\n      <td>4.96</td>\n      <td>4.92</td>\n      <td>4.98</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.927000e+03</td>\n      <td>2009-04-30</td>\n      <td>14100.0</td>\n      <td>Melissa</td>\n      <td>I can't say enough about how wonderful it was ...</td>\n      <td>i cant say enough about how wonderful it was t...</td>\n      <td>['not', 'wonderful', 'stay', 'highlight', 'sta...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>12172</td>\n      <td>6422</td>\n      <td>4.95</td>\n      <td>4.94</td>\n      <td>4.96</td>\n      <td>4.97</td>\n      <td>4.96</td>\n      <td>4.92</td>\n      <td>4.98</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>3.867000e+03</td>\n      <td>2009-06-11</td>\n      <td>17413.0</td>\n      <td>Raquel</td>\n      <td>Michelle and Collier's home is wonderful! They...</td>\n      <td>michelle and colliers home is wonderful they a...</td>\n      <td>['michelle', 'collier', 'home', 'wonderful', '...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>12172</td>\n      <td>6422</td>\n      <td>4.95</td>\n      <td>4.94</td>\n      <td>4.96</td>\n      <td>4.97</td>\n      <td>4.96</td>\n      <td>4.92</td>\n      <td>4.98</td>\n      <td>2.0</td>\n      <td>2.0</td>\n      <td>4.159000e+03</td>\n      <td>2009-06-17</td>\n      <td>20253.0</td>\n      <td>Ulrike</td>\n      <td>I spent one night at Michele's home and felt j...</td>\n      <td>i spent one night at micheles home and felt ju...</td>\n      <td>['spend', 'night', 'micheles', 'home', 'feel',...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>12172</td>\n      <td>6422</td>\n      <td>4.95</td>\n      <td>4.94</td>\n      <td>4.96</td>\n      <td>4.97</td>\n      <td>4.96</td>\n      <td>4.92</td>\n      <td>4.98</td>\n      <td>3.0</td>\n      <td>3.0</td>\n      <td>5.724000e+03</td>\n      <td>2009-07-18</td>\n      <td>22544.0</td>\n      <td>Phil</td>\n      <td>Michele and Collier are two of the loveliest p...</td>\n      <td>michele and collier are two of the loveliest p...</td>\n      <td>['michele', 'collier', 'lovely', 'people', 'pl...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>12172</td>\n      <td>6422</td>\n      <td>4.95</td>\n      <td>4.94</td>\n      <td>4.96</td>\n      <td>4.97</td>\n      <td>4.96</td>\n      <td>4.92</td>\n      <td>4.98</td>\n      <td>4.0</td>\n      <td>4.0</td>\n      <td>1.189100e+04</td>\n      <td>2009-09-29</td>\n      <td>33409.0</td>\n      <td>Claire</td>\n      <td>We had the most lovely time staying with Miche...</td>\n      <td>we had the most lovely time staying with miche...</td>\n      <td>['lovely', 'time', 'stay', 'michele', 'colly',...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>10717282</th>\n      <td>31058129</td>\n      <td>903053207440059523</td>\n      <td>5.00</td>\n      <td>5.00</td>\n      <td>5.00</td>\n      <td>5.00</td>\n      <td>5.00</td>\n      <td>5.00</td>\n      <td>5.00</td>\n      <td>9668038.0</td>\n      <td>1459398.0</td>\n      <td>9.059651e+17</td>\n      <td>2023-06-03</td>\n      <td>266498612.0</td>\n      <td>Paul</td>\n      <td>Jordan is a great host, is super communicative...</td>\n      <td>jordan is a great host is super communicative ...</td>\n      <td>['jordan', 'great', 'host', 'super', 'communic...</td>\n    </tr>\n    <tr>\n      <th>10717283</th>\n      <td>31058129</td>\n      <td>903053207440059523</td>\n      <td>5.00</td>\n      <td>5.00</td>\n      <td>5.00</td>\n      <td>5.00</td>\n      <td>5.00</td>\n      <td>5.00</td>\n      <td>5.00</td>\n      <td>9668039.0</td>\n      <td>1459399.0</td>\n      <td>9.073184e+17</td>\n      <td>2023-06-05</td>\n      <td>754306.0</td>\n      <td>Katerine</td>\n      <td>I had an absolutely wonderful stay at Jordan’s...</td>\n      <td>i had an absolutely wonderful stay at jordans ...</td>\n      <td>['absolutely', 'wonderful', 'stay', 'jordans',...</td>\n    </tr>\n    <tr>\n      <th>10717321</th>\n      <td>134981485</td>\n      <td>904459169206385872</td>\n      <td>5.00</td>\n      <td>5.00</td>\n      <td>5.00</td>\n      <td>5.00</td>\n      <td>5.00</td>\n      <td>5.00</td>\n      <td>5.00</td>\n      <td>9668040.0</td>\n      <td>1459400.0</td>\n      <td>9.066321e+17</td>\n      <td>2023-06-04</td>\n      <td>233078624.0</td>\n      <td>Chien-Hao</td>\n      <td>A very smooth stay in this place. The host is ...</td>\n      <td>a very smooth stay in this place the host is v...</td>\n      <td>['smooth', 'stay', 'place', 'host', 'responsiv...</td>\n    </tr>\n    <tr>\n      <th>10717359</th>\n      <td>364080225</td>\n      <td>903836186965131941</td>\n      <td>5.00</td>\n      <td>5.00</td>\n      <td>5.00</td>\n      <td>5.00</td>\n      <td>5.00</td>\n      <td>5.00</td>\n      <td>5.00</td>\n      <td>9668036.0</td>\n      <td>1459396.0</td>\n      <td>9.066552e+17</td>\n      <td>2023-06-04</td>\n      <td>73264277.0</td>\n      <td>Catherine</td>\n      <td>Nice private room with private bathroom. Easy ...</td>\n      <td>nice private room with private bathroom easy t...</td>\n      <td>['nice', 'private', 'room', 'private', 'bathro...</td>\n    </tr>\n    <tr>\n      <th>10717538</th>\n      <td>24594764</td>\n      <td>905798817363097610</td>\n      <td>5.00</td>\n      <td>5.00</td>\n      <td>5.00</td>\n      <td>5.00</td>\n      <td>5.00</td>\n      <td>5.00</td>\n      <td>5.00</td>\n      <td>9668044.0</td>\n      <td>1459404.0</td>\n      <td>9.073806e+17</td>\n      <td>2023-06-05</td>\n      <td>277166716.0</td>\n      <td>Lionel</td>\n      <td>Omg. This place was absolutely amazing. Very s...</td>\n      <td>omg this place was absolutely amazing very spa...</td>\n      <td>['omg', 'place', 'absolutely', 'amazing', 'spa...</td>\n    </tr>\n  </tbody>\n</table>\n<p>10618706 rows × 18 columns</p>\n</div>"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_revs"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "outputs": [],
   "source": [
    "import xgboost\n",
    "\n",
    "#regressor = XGBRegressor(device='cuda',tree_method='hist')\n",
    "#regressor = XGBRegressor(objective='reg:squarederror')\n",
    "regressor = XGBRegressor()\n",
    "beg_slice = 0\n",
    "end_slice = 1000"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.697803020477295\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "vectorizer = TfidfVectorizer(analyzer='word',ngram_range=(1,4))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_revs['cleaned_text'][beg_slice:end_slice],df_revs['review_scores_rating'][beg_slice:end_slice],test_size=0.2,random_state=42)\n",
    "\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = vectorizer.transform(X_test)\n",
    "\n",
    "#regressor = RandomForestRegressor()\n",
    "\n",
    "regressor.fit(X_train_tfidf, y_train)\n",
    "\n",
    "y_pred = regressor.predict(X_test_tfidf)\n",
    "\n",
    "mse = mean_squared_error(y_test,y_pred)\n",
    "\n",
    "end_time = time.time()\n",
    "print(end_time - start_time)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%timeit -r 1 -n 1\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "outputs": [
    {
     "data": {
      "text/plain": "8.088689251055205e-06"
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mse"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "outputs": [
    {
     "data": {
      "text/plain": "              Feature  Feature_Weights\n554625          bobby         0.023559\n1798424          kate         0.017451\n547390          blake         0.016789\n547788         blakes         0.010945\n2019894          mike         0.010062\n1799017         kates         0.009679\n1989276          mary         0.007950\n957873         donald         0.007246\n870726         daphne         0.007093\n885182          debra         0.006685\n935763          dirty         0.006337\n2015782      michelle         0.006188\n1803706         kenny         0.005306\n1930954       lorenas         0.005269\n3239538          tiny         0.004878\n1931030          lori         0.004822\n2015795  michelle and         0.004817\n1930851        lorena         0.004611\n2036459         mitch         0.004600\n869993          danny         0.004552\n573132           brad         0.004325\n1839788        lauren         0.004154\n3351101       touches         0.004132\n1784180        joseph         0.004070\n871007        daphnes         0.003968\n3225428       tiffany         0.003810\n2197618            no         0.003744\n1783789        jonnys         0.003698\n2018041       midtown         0.003696\n2398523           pam         0.003694",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Feature</th>\n      <th>Feature_Weights</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>554625</th>\n      <td>bobby</td>\n      <td>0.023559</td>\n    </tr>\n    <tr>\n      <th>1798424</th>\n      <td>kate</td>\n      <td>0.017451</td>\n    </tr>\n    <tr>\n      <th>547390</th>\n      <td>blake</td>\n      <td>0.016789</td>\n    </tr>\n    <tr>\n      <th>547788</th>\n      <td>blakes</td>\n      <td>0.010945</td>\n    </tr>\n    <tr>\n      <th>2019894</th>\n      <td>mike</td>\n      <td>0.010062</td>\n    </tr>\n    <tr>\n      <th>1799017</th>\n      <td>kates</td>\n      <td>0.009679</td>\n    </tr>\n    <tr>\n      <th>1989276</th>\n      <td>mary</td>\n      <td>0.007950</td>\n    </tr>\n    <tr>\n      <th>957873</th>\n      <td>donald</td>\n      <td>0.007246</td>\n    </tr>\n    <tr>\n      <th>870726</th>\n      <td>daphne</td>\n      <td>0.007093</td>\n    </tr>\n    <tr>\n      <th>885182</th>\n      <td>debra</td>\n      <td>0.006685</td>\n    </tr>\n    <tr>\n      <th>935763</th>\n      <td>dirty</td>\n      <td>0.006337</td>\n    </tr>\n    <tr>\n      <th>2015782</th>\n      <td>michelle</td>\n      <td>0.006188</td>\n    </tr>\n    <tr>\n      <th>1803706</th>\n      <td>kenny</td>\n      <td>0.005306</td>\n    </tr>\n    <tr>\n      <th>1930954</th>\n      <td>lorenas</td>\n      <td>0.005269</td>\n    </tr>\n    <tr>\n      <th>3239538</th>\n      <td>tiny</td>\n      <td>0.004878</td>\n    </tr>\n    <tr>\n      <th>1931030</th>\n      <td>lori</td>\n      <td>0.004822</td>\n    </tr>\n    <tr>\n      <th>2015795</th>\n      <td>michelle and</td>\n      <td>0.004817</td>\n    </tr>\n    <tr>\n      <th>1930851</th>\n      <td>lorena</td>\n      <td>0.004611</td>\n    </tr>\n    <tr>\n      <th>2036459</th>\n      <td>mitch</td>\n      <td>0.004600</td>\n    </tr>\n    <tr>\n      <th>869993</th>\n      <td>danny</td>\n      <td>0.004552</td>\n    </tr>\n    <tr>\n      <th>573132</th>\n      <td>brad</td>\n      <td>0.004325</td>\n    </tr>\n    <tr>\n      <th>1839788</th>\n      <td>lauren</td>\n      <td>0.004154</td>\n    </tr>\n    <tr>\n      <th>3351101</th>\n      <td>touches</td>\n      <td>0.004132</td>\n    </tr>\n    <tr>\n      <th>1784180</th>\n      <td>joseph</td>\n      <td>0.004070</td>\n    </tr>\n    <tr>\n      <th>871007</th>\n      <td>daphnes</td>\n      <td>0.003968</td>\n    </tr>\n    <tr>\n      <th>3225428</th>\n      <td>tiffany</td>\n      <td>0.003810</td>\n    </tr>\n    <tr>\n      <th>2197618</th>\n      <td>no</td>\n      <td>0.003744</td>\n    </tr>\n    <tr>\n      <th>1783789</th>\n      <td>jonnys</td>\n      <td>0.003698</td>\n    </tr>\n    <tr>\n      <th>2018041</th>\n      <td>midtown</td>\n      <td>0.003696</td>\n    </tr>\n    <tr>\n      <th>2398523</th>\n      <td>pam</td>\n      <td>0.003694</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "features_df = pd.DataFrame({'Feature': feature_names,'Feature_Weights': regressor.feature_importances_})\n",
    "features_df.sort_values(by='Feature_Weights',ascending=False)[:30]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\base.py:1152\u001B[0m, in \u001B[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001B[1;34m(estimator, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1147\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m config_context(\n\u001B[0;32m   1148\u001B[0m     skip_parameter_validation\u001B[38;5;241m=\u001B[39m(\n\u001B[0;32m   1149\u001B[0m         prefer_skip_nested_validation \u001B[38;5;129;01mor\u001B[39;00m global_skip_validation\n\u001B[0;32m   1150\u001B[0m     )\n\u001B[0;32m   1151\u001B[0m ):\n\u001B[1;32m-> 1152\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m fit_method(estimator, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1389\u001B[0m, in \u001B[0;36mCountVectorizer.fit_transform\u001B[1;34m(self, raw_documents, y)\u001B[0m\n\u001B[0;32m   1387\u001B[0m             \u001B[38;5;28;01mbreak\u001B[39;00m\n\u001B[1;32m-> 1389\u001B[0m vocabulary, X \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_count_vocab\u001B[49m\u001B[43m(\u001B[49m\u001B[43mraw_documents\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfixed_vocabulary_\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1391\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbinary:\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1313\u001B[0m, in \u001B[0;36mCountVectorizer._count_vocab\u001B[1;34m(self, raw_documents, fixed_vocab)\u001B[0m\n\u001B[0;32m   1312\u001B[0m j_indices \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39masarray(j_indices, dtype\u001B[38;5;241m=\u001B[39mindices_dtype)\n\u001B[1;32m-> 1313\u001B[0m indptr \u001B[38;5;241m=\u001B[39m \u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43masarray\u001B[49m\u001B[43m(\u001B[49m\u001B[43mindptr\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdtype\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mindices_dtype\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1314\u001B[0m values \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mfrombuffer(values, dtype\u001B[38;5;241m=\u001B[39mnp\u001B[38;5;241m.\u001B[39mintc)\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[130], line 13\u001B[0m\n\u001B[0;32m     10\u001B[0m X_train, X_test, y_train, y_test \u001B[38;5;241m=\u001B[39m train_test_split(X,y,test_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.2\u001B[39m,random_state\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m42\u001B[39m)\n\u001B[0;32m     11\u001B[0m X_train, X_val, y_train, y_val \u001B[38;5;241m=\u001B[39m train_test_split(X_train,y_train,test_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.2\u001B[39m,random_state\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m42\u001B[39m)\n\u001B[1;32m---> 13\u001B[0m X_train_tfidf \u001B[38;5;241m=\u001B[39m \u001B[43mtfidf_vectorizer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit_transform\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX_train\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     14\u001B[0m X_test_tfidf \u001B[38;5;241m=\u001B[39m tfidf_vectorizer\u001B[38;5;241m.\u001B[39mtransform(X_test)\n\u001B[0;32m     15\u001B[0m X_val_tfidf \u001B[38;5;241m=\u001B[39m tfidf_vectorizer\u001B[38;5;241m.\u001B[39mtransform(X_val)\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:2139\u001B[0m, in \u001B[0;36mTfidfVectorizer.fit_transform\u001B[1;34m(self, raw_documents, y)\u001B[0m\n\u001B[0;32m   2132\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_check_params()\n\u001B[0;32m   2133\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_tfidf \u001B[38;5;241m=\u001B[39m TfidfTransformer(\n\u001B[0;32m   2134\u001B[0m     norm\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnorm,\n\u001B[0;32m   2135\u001B[0m     use_idf\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39muse_idf,\n\u001B[0;32m   2136\u001B[0m     smooth_idf\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msmooth_idf,\n\u001B[0;32m   2137\u001B[0m     sublinear_tf\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msublinear_tf,\n\u001B[0;32m   2138\u001B[0m )\n\u001B[1;32m-> 2139\u001B[0m X \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit_transform\u001B[49m\u001B[43m(\u001B[49m\u001B[43mraw_documents\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   2140\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_tfidf\u001B[38;5;241m.\u001B[39mfit(X)\n\u001B[0;32m   2141\u001B[0m \u001B[38;5;66;03m# X is already a transformed view of raw_documents so\u001B[39;00m\n\u001B[0;32m   2142\u001B[0m \u001B[38;5;66;03m# we set copy to False\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\base.py:1147\u001B[0m, in \u001B[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001B[1;34m(estimator, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1144\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m global_skip_validation \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m partial_fit_and_fitted:\n\u001B[0;32m   1145\u001B[0m     estimator\u001B[38;5;241m.\u001B[39m_validate_params()\n\u001B[1;32m-> 1147\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m config_context(\n\u001B[0;32m   1148\u001B[0m     skip_parameter_validation\u001B[38;5;241m=\u001B[39m(\n\u001B[0;32m   1149\u001B[0m         prefer_skip_nested_validation \u001B[38;5;129;01mor\u001B[39;00m global_skip_validation\n\u001B[0;32m   1150\u001B[0m     )\n\u001B[0;32m   1151\u001B[0m ):\n\u001B[0;32m   1152\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m fit_method(estimator, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\contextlib.py:139\u001B[0m, in \u001B[0;36m_GeneratorContextManager.__exit__\u001B[1;34m(self, typ, value, traceback)\u001B[0m\n\u001B[0;32m    136\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mStopIteration\u001B[39;00m:\n\u001B[0;32m    137\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mgenerator didn\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mt yield\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m--> 139\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__exit__\u001B[39m(\u001B[38;5;28mself\u001B[39m, typ, value, traceback):\n\u001B[0;32m    140\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m typ \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    141\u001B[0m         \u001B[38;5;28;01mtry\u001B[39;00m:\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "xgb_model = XGBRegressor(device='cuda',tree_method='hist')\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(analyzer='word',ngram_range=(1,4))\n",
    "\n",
    "X = df_revs['cleaned_text']\n",
    "y = df_revs['review_scores_rating']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2,random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train,y_train,test_size=0.2,random_state=42)\n",
    "\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
    "X_val_tfidf = tfidf_vectorizer.transform(X_val)\n",
    "\n",
    "X_test_gpu = cp.array(X_test_tfidf.toarray())\n",
    "X_val_gpu = cp.array(X_val_tfidf.toarray())\n",
    "y_test_gpu = cp.array(y_test)\n",
    "y_val_gpu = cp.array(y_val)\n",
    "\n",
    "\n",
    "batch_size = 10000\n",
    "\n",
    "num_batches = int(np.ceil(X_train_tfidf.shape[0] / batch_size))\n",
    "\n",
    "start_idx = 0\n",
    "end_idx = min(batch_size, X_train_tfidf.shape[0])\n",
    "\n",
    "X_train_batch_gpu = cp.array(X_train_tfidf[start_idx:end_idx].toarray())\n",
    "y_train_batch_gpu = cp.array(y_train[start_idx:end_idx])\n",
    "xgb_model.fit(X_train_batch_gpu, y_train_batch_gpu,eval_set=[(X_val_gpu,y_val_gpu)],eval_metric='rmse',early_stopping_rounds=10,verbose=True)\n",
    "\n",
    "print(\"Initial model fitting complete...\")\n",
    "\n",
    "for i in range(1,num_batches):\n",
    "    print(\"Beginning to iterate over batches\")\n",
    "    start_idx = i * batch_size\n",
    "    end_idx = min((i+1) * batch_size, X_train_tfidf.shape[0])\n",
    "\n",
    "    X_train_batch_gpu = cp.array(X_train_tfidf[start_idx:end_idx].toarray())\n",
    "    y_train_batch_gpu = cp.array(y_train[start_idx:end_idx])\n",
    "\n",
    "    xgb_model.fit(X_train_batch_gpu,y_train_batch_gpu,xgb_model.get_booster().best_iteration,eval_set=[(X_val_gpu,y_val_gpu)],eval_metric='rmse',early_stopping_rounds=10,verbose=True)\n",
    "\n",
    "    y_pred_gpu = xgb_model.predict(X_test_gpu)\n",
    "\n",
    "    rmse = np.sqrt(mean_squared_error(y_test_gpu.get(), y_pred_gpu))\n",
    "    print(f\"Batch {i+1}/{num_batches} - RMSE: {rmse}\")\n",
    "\n",
    "y_pred_gpu = xgb_model.predict(X_test_gpu)\n",
    "final_rmse = np.sqrt(mean_squared_error(y_test_gpu.get(), y_pred_gpu))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "xgb_model = XGBRegressor(device='cuda',tree_method='hist')\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(analyzer='word',ngram_range=(1,4))\n",
    "\n",
    "X = df_revs['cleaned_text']\n",
    "y = df_revs['review_scores_rating']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2,random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train,y_train,test_size=0.2,random_state=42)\n",
    "\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
    "X_val_tfidf = tfidf_vectorizer.transform(X_val)\n",
    "\n",
    "X_test_gpu = cp.array(X_test_tfidf.toarray())\n",
    "X_val_gpu = cp.array(X_val_tfidf.toarray())\n",
    "y_test_gpu = cp.array(y_test)\n",
    "y_val_gpu = cp.array(y_val)\n",
    "\n",
    "\n",
    "batch_size = 10000\n",
    "\n",
    "num_batches = int(np.ceil(X_train_tfidf.shape[0] / batch_size))\n",
    "\n",
    "start_idx = 0\n",
    "end_idx = min(batch_size, X_train_tfidf.shape[0])\n",
    "\n",
    "X_train_batch_gpu = cp.array(X_train_tfidf[start_idx:end_idx].toarray())\n",
    "y_train_batch_gpu = cp.array(y_train[start_idx:end_idx])\n",
    "xgb_model.fit(X_train_batch_gpu, y_train_batch_gpu,eval_set=[(X_val_gpu,y_val_gpu)],eval_metric='rmse',early_stopping_rounds=10,verbose=True)\n",
    "\n",
    "print(\"Initial model fitting complete...\")\n",
    "\n",
    "for i in range(1,num_batches):\n",
    "    print(\"Beginning to iterate over batches\")\n",
    "    start_idx = i * batch_size\n",
    "    end_idx = min((i+1) * batch_size, X_train_tfidf.shape[0])\n",
    "\n",
    "    X_train_batch_gpu = cp.array(X_train_tfidf[start_idx:end_idx].toarray())\n",
    "    y_train_batch_gpu = cp.array(y_train[start_idx:end_idx])\n",
    "\n",
    "    xgb_model.fit(X_train_batch_gpu,y_train_batch_gpu,xgb_model.get_booster().best_iteration,eval_set=[(X_val_gpu,y_val_gpu)],eval_metric='rmse',early_stopping_rounds=10,verbose=True)\n",
    "\n",
    "    y_pred_gpu = xgb_model.predict(X_test_gpu)\n",
    "\n",
    "    rmse = np.sqrt(mean_squared_error(y_test_gpu.get(), y_pred_gpu))\n",
    "    print(f\"Batch {i+1}/{num_batches} - RMSE: {rmse}\")\n",
    "\n",
    "y_pred_gpu = xgb_model.predict(X_test_gpu)\n",
    "final_rmse = np.sqrt(mean_squared_error(y_test_gpu.get(), y_pred_gpu))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "outputs": [],
   "source": [
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "def read_data_in_batches(file_path,read_batch_size,file_type):\n",
    "    if file_type == 'csv':\n",
    "        reader = pd.read_csv(file_path,chunksize=read_batch_size)\n",
    "        for batch_df in reader:\n",
    "            yield batch_df\n",
    "    if file_type == 'parquet':\n",
    "        pd.read_parquet(file_path,chunksize=read_batch_size)\n",
    "        for batch_df in reader:\n",
    "            yield batch_df\n",
    "    else:\n",
    "        \"No file type specified. Please specify either csv or parquet\"\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "Out of memory allocating 2,044,680,192 bytes (allocated so far: 14,960,867,840 bytes).",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mOutOfMemoryError\u001B[0m                          Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[14], line 29\u001B[0m\n\u001B[0;32m     26\u001B[0m X_test_tfidf \u001B[38;5;241m=\u001B[39m tfidf_vectorizer\u001B[38;5;241m.\u001B[39mtransform(X_test)\n\u001B[0;32m     27\u001B[0m X_val_tfidf \u001B[38;5;241m=\u001B[39m tfidf_vectorizer\u001B[38;5;241m.\u001B[39mtransform(X_val)\n\u001B[1;32m---> 29\u001B[0m X_test_gpu \u001B[38;5;241m=\u001B[39m \u001B[43mcp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43marray\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX_test_tfidf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtoarray\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     30\u001B[0m X_val_gpu \u001B[38;5;241m=\u001B[39m cp\u001B[38;5;241m.\u001B[39marray(X_val_tfidf\u001B[38;5;241m.\u001B[39mtoarray())\n\u001B[0;32m     31\u001B[0m y_test_gpu \u001B[38;5;241m=\u001B[39m cp\u001B[38;5;241m.\u001B[39marray(y_test)\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\cupy\\_creation\\from_data.py:53\u001B[0m, in \u001B[0;36marray\u001B[1;34m(obj, dtype, copy, order, subok, ndmin, blocking)\u001B[0m\n\u001B[0;32m      7\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21marray\u001B[39m(obj, dtype\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, copy\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m, order\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mK\u001B[39m\u001B[38;5;124m'\u001B[39m, subok\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m, ndmin\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m*\u001B[39m,\n\u001B[0;32m      8\u001B[0m           blocking\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m):\n\u001B[0;32m      9\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Creates an array on the current device.\u001B[39;00m\n\u001B[0;32m     10\u001B[0m \n\u001B[0;32m     11\u001B[0m \u001B[38;5;124;03m    This function currently does not support the ``subok`` option.\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     51\u001B[0m \n\u001B[0;32m     52\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m---> 53\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_core\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43marray\u001B[49m\u001B[43m(\u001B[49m\u001B[43mobj\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdtype\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcopy\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43morder\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msubok\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mndmin\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mblocking\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mcupy\\_core\\core.pyx:2379\u001B[0m, in \u001B[0;36mcupy._core.core.array\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32mcupy\\_core\\core.pyx:2406\u001B[0m, in \u001B[0;36mcupy._core.core.array\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32mcupy\\_core\\core.pyx:2548\u001B[0m, in \u001B[0;36mcupy._core.core._array_default\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32mcupy\\_core\\core.pyx:132\u001B[0m, in \u001B[0;36mcupy._core.core.ndarray.__new__\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32mcupy\\_core\\core.pyx:220\u001B[0m, in \u001B[0;36mcupy._core.core._ndarray_base._init\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32mcupy\\cuda\\memory.pyx:738\u001B[0m, in \u001B[0;36mcupy.cuda.memory.alloc\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32mcupy\\cuda\\memory.pyx:1424\u001B[0m, in \u001B[0;36mcupy.cuda.memory.MemoryPool.malloc\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32mcupy\\cuda\\memory.pyx:1445\u001B[0m, in \u001B[0;36mcupy.cuda.memory.MemoryPool.malloc\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32mcupy\\cuda\\memory.pyx:1116\u001B[0m, in \u001B[0;36mcupy.cuda.memory.SingleDeviceMemoryPool.malloc\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32mcupy\\cuda\\memory.pyx:1137\u001B[0m, in \u001B[0;36mcupy.cuda.memory.SingleDeviceMemoryPool._malloc\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32mcupy\\cuda\\memory.pyx:1382\u001B[0m, in \u001B[0;36mcupy.cuda.memory.SingleDeviceMemoryPool._try_malloc\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32mcupy\\cuda\\memory.pyx:1385\u001B[0m, in \u001B[0;36mcupy.cuda.memory.SingleDeviceMemoryPool._try_malloc\u001B[1;34m()\u001B[0m\n",
      "\u001B[1;31mOutOfMemoryError\u001B[0m: Out of memory allocating 2,044,680,192 bytes (allocated so far: 14,960,867,840 bytes)."
     ]
    }
   ],
   "source": [
    "parquet_file_path = r\"F:\\Data Science\\Datasets\\AirBnB Reviews Regression Inputs\\combined_processed_reviews.parquet\"\n",
    "csv_file_path = r\"F:\\Data Science\\Datasets\\AirBnB Reviews Regression Inputs\\combined_processed_reviews.csv\"\n",
    "\n",
    "file_type = 'csv'\n",
    "if file_type == 'parquet':\n",
    "    file_path = r\"F:\\Data Science\\Datasets\\AirBnB Reviews Regression Inputs\\combined_processed_reviews.parquet\"\n",
    "if file_type == 'csv':\n",
    "    file_path = r\"F:\\Data Science\\Datasets\\AirBnB Reviews Regression Inputs\\combined_processed_reviews.csv\"\n",
    "\n",
    "read_batch_size = 5000\n",
    "batch_size = 1000\n",
    "\n",
    "xgb_model = XGBRegressor(device='cuda',tree_method='hist')\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(analyzer='word',ngram_range=(1,4))\n",
    "\n",
    "\n",
    "for b, batch_df in enumerate(read_data_in_batches(file_path=file_path, read_batch_size=read_batch_size, file_type=file_type)):\n",
    "    X = batch_df['cleaned_text']\n",
    "    y = batch_df['review_scores_rating']\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2,random_state=42)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train,y_train,test_size=0.2,random_state=42)\n",
    "\n",
    "    X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "    X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
    "    X_val_tfidf = tfidf_vectorizer.transform(X_val)\n",
    "\n",
    "    X_test_gpu = cp.array(X_test_tfidf.toarray())\n",
    "    X_val_gpu = cp.array(X_val_tfidf.toarray())\n",
    "    y_test_gpu = cp.array(y_test)\n",
    "    y_val_gpu = cp.array(y_val)\n",
    "\n",
    "\n",
    "    num_batches = int(np.ceil(X_train_tfidf.shape[0] / batch_size))\n",
    "\n",
    "    start_idx = 0\n",
    "    end_idx = min(batch_size, X_train_tfidf.shape[0])\n",
    "\n",
    "    X_train_batch_gpu = cp.array(X_train_tfidf[start_idx:end_idx].toarray())\n",
    "    y_train_batch_gpu = cp.array(y_train[start_idx:end_idx])\n",
    "\n",
    "    if b == 0:\n",
    "        xgb_model.fit(X_train_batch_gpu, y_train_batch_gpu,eval_set=[(X_val_gpu,y_val_gpu)],eval_metric='rmse',early_stopping_rounds=10,verbose=True)\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "    for i in range(1,num_batches):\n",
    "        print(\"Beginning to iterate over batches\")\n",
    "        start_idx = i * batch_size\n",
    "        end_idx = min((i+1) * batch_size, X_train_tfidf.shape[0])\n",
    "\n",
    "        X_train_batch_gpu = cp.array(X_train_tfidf[start_idx:end_idx].toarray())\n",
    "        y_train_batch_gpu = cp.array(y_train[start_idx:end_idx])\n",
    "\n",
    "        xgb_model.fit(X_train_batch_gpu,y_train_batch_gpu,xgb_model.get_booster().best_iteration,eval_set=[(X_val_gpu,y_val_gpu)],eval_metric='rmse',early_stopping_rounds=10,verbose=True)\n",
    "\n",
    "        y_pred_gpu = xgb_model.predict(X_test_gpu)\n",
    "\n",
    "        rmse = np.sqrt(mean_squared_error(y_test_gpu.get(), y_pred_gpu))\n",
    "        print(f\"Batch {i+1}/{num_batches} - RMSE: {rmse}\")\n",
    "\n",
    "    print('MODEL EVAL FOR THIS BATCH')\n",
    "    y_pred_gpu = xgb_model.predict(X_test_gpu)\n",
    "    final_rmse = np.sqrt(mean_squared_error(y_test_gpu.get(), y_pred_gpu))\n",
    "    print(final_rmse)\n",
    "\n",
    "if file_type == 'csv':\n",
    "    df_revs = pd.read_csv(file_path)\n",
    "if file_type == 'parquet':\n",
    "    df_revs = pd.read_parquet(file_path)\n",
    "\n",
    "X = df_revs['cleaned_text']\n",
    "y = df_revs['review_scores_rating']\n",
    "\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X)\n",
    "X_test_gpu = cp.array(X_test_tfidf.toarray())\n",
    "y_test_gpu = cp.array(y)\n",
    "\n",
    "y_pred_gpu = xgb_model.predict(X_test_gpu)\n",
    "final_rmse = np.sqrt(mean_squared_error(y_test_gpu.get(), y_pred_gpu))\n",
    "print(f\"Final RMSE: {final_rmse}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
